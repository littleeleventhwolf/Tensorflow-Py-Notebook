{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词向量和语言模型\n",
    "\n",
    "在了解TensorFlow生成词向量(Word Embedding)之前，先来认识一些概念。\n",
    "\n",
    "将词用\"词向量\"的方式表示可谓是将Deep Learning算法引入NLP领域的一个核心技术。大多数宣称用了Deep Learning的论文，其中往往也用了词向量。\n",
    "\n",
    "## 0.词向量是什么\n",
    "\n",
    "自然语言理解的问题要转化为机器学习的问题，第一步肯定是要找一种方法把这些符号数学化。\n",
    "\n",
    "NLP中最直观，也是目前为止最常用的词表示方法是One-hot Representation，这种方法把每个词表示为一个很长的向量。这个向量的维度是词表的大小，其中绝大多数元素是0，只有一个维度的值为1，这个维度就代表了当前的词。\n",
    "\n",
    "举个例子：\n",
    "\n",
    "\"话筒\"表示为[0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 ...]\n",
    "\"麦克\"表示为[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 ...]\n",
    "\n",
    "每个词都是茫茫0海中的一个1。\n",
    "\n",
    "这种One-hot Representation如果采用稀疏方式存储，会是非常的简洁：也就是说每个词分配一个数字ID。比如刚才的例子中，\"话筒\"记为3，\"麦克\"记为8(假设从0开始记)。如果要编程实现的话，用Hash表给每个词分配一个编号就可以了。这么简洁的方式配合上最大熵、SVM、CRF等等算法已经很好地完成了NLP领域的各种主流任务。\n",
    "\n",
    "当然这种方法也存在一个重要的问题就是\"词汇鸿沟\"现象：任意两个词之间都是孤立的。光从这两个向量中看不出两个词是否存在关系，哪怕是\"话筒\"和\"麦克\"这样的同义词也不能幸免于难。\n",
    "\n",
    "Deep Learning中一般用到的词向量并不是刚才提到的用One-hot Representation表示的那种很长的词向量，而是用Distributed Representation(不知道这个该怎么翻译，因为还存在一种叫做\"Distributional Representation\"的表示方法，又是另一个不同的概念)表示的一种低维实数向量。这种向量一般长成这个样子：[0.792, -0.177, -0.107, 0.109, -0.524, ...]。维度以50维和100维比较常见。这种向量的表示不是唯一的，后文会提到目前计算出这种向量的主流方法。\n",
    "\n",
    "Distributed Representation最大的贡献就是让相关或者相似的词，在距离上更接近了。向量的距离可以用最传统的欧氏距离来衡量，也可以用cos夹角来衡量。用这种方式表示的向量，\"麦克\"和\"话筒\"的距离会远远小于\"麦克\"和\"天气\"。可能理想情况下，\"麦克\"和\"话筒\"的表示应该是完全一样的，但是由于有些人会把英文名\"迈克\"也写成\"麦克\"，导致\"麦克\"一词带上了一些人名的语义，因此不会和\"话筒\"完全一致。\n",
    "\n",
    "## 1.词向量的来历\n",
    "\n",
    "Distributed Representation最早是Hinton在1986年的论文《Learning distribued representations of concepts》中提出的。虽然这篇文章没有说要将词做Distributed Representation，但至少这种先进的思想在那个时候就在人们的心中埋下了火种，到2000年之后开始逐渐被人重视。\n",
    "\n",
    "Distributed Representation用来表示词，通常被称为\"Word Representation\"或\"Word Embeding\"，中文俗称\"词向量\"。后文提到的所有词向量都是指用Distributed Representation表示的词向量。\n",
    "\n",
    "如果用传统的稀疏表示法表示词，在解决某些任务的时候(比如构建语言模型)会造成维数灾难。使用低维的词向量就没有这样的问题。同时从实践上看，高维的特征如果要套用Deep Learnin，其复杂度几乎是难以接受的，因此低维的词向量在这里也饱受追捧。\n",
    "\n",
    "同时如上一小节提到的，相似词的词向量距离相近，这就让基于词向量设计的一些模型自带平滑功能，让模型看起来非常的漂亮。\n",
    "\n",
    "## 2.词向量的训练\n",
    "\n",
    "要介绍词向量是怎么训练得到的，就不得不提语言模型。到目前为止我了解到的所有训练方法都是在训练语言模型的同时，顺便提到词向量的。\n",
    "\n",
    "这也比较容易理解，要从一段无标注的自然文本中学习出一些东西，无非就是统计出词频、词的共现、词的搭配之类的信息。而要从自然文本中统计并建立一个语言模型，无疑是要求最为精准的一个任务。既然构建语言模型这一任务要求这么高，其中必然也需要对语言进行更精细的统计和分析，同时也会需要更好的模型，更大的数据来支撑。目前最好的词向量都来自于此，也就不难理解了。\n",
    "\n",
    "这里介绍的工作均为从大量未标注的普通文本数据中无监督地学习出词向量(语言模型本来就是基于这个想法而来的)，可以猜测，如果用上了有标注的语料，训练词向量的方法肯定会更多。不过视目前的语料规模，还是使用未标注的方法靠谱一点。\n",
    "\n",
    "词向量的训练最经典的有3个工作，C&W 2008、M&H 2008、Mikolov 2010。\n",
    "\n",
    "### 2.0 语言模型简介\n",
    "\n",
    "简单介绍一下语言模型。语言模型其实就是看一句话是不是正常人说出来的。这玩意很有用，比如机器翻译、语音识别得到若干候选之后，可以利用语言模型挑一个尽量靠谱的结果。在NLP的其他任务里也能用到。\n",
    "\n",
    "语言模型形式化得描述就是给定一个字符串，看它是自然语言的概率P(w1,w2,…,wt)。w1到wt依次表示这句话中的各个词。有个很简单的推论是：\n",
    "\n",
    "P(w1,w2,…,wt)=P(w1)×P(w2|w1)×P(w3|w1,w2)×…×P(wt|w1,w2,…,wt−1)\n",
    "\n",
    "常用的语言模型都是在近似地求P(wt|w1,w2,…,wt−1)。比如n-gram模型就是用P(wt|wt−n+1,…,wt−1)近似表示前者。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
